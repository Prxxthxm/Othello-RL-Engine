{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_action(e, state) -> tuple[int, int]:\n",
    "    pass\n",
    "\n",
    "\n",
    "def train():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, dones, next_states = zip(*batch)\n",
    "\n",
    "    states = torch.stack(states).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64, device=device).reshape(batch_size)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).reshape(batch_size).to(device)  # Convert rewards to a tensor\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).reshape(batch_size).to(device)  # Convert dones to a tensor\n",
    "\n",
    "    q_values = model(states)\n",
    "    predicted_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # get the q values for the corresponding actions taken\n",
    "\n",
    "    next_q_values = torch.zeros(batch_size, device=device)\n",
    "\n",
    "    non_terminal_mask = (dones == 0)  # Mask for non-terminal states\n",
    "    non_terminal_next_states = [s for s in next_states if s is not None]\n",
    "\n",
    "    if non_terminal_next_states:\n",
    "        non_terminal_next_states = torch.stack(non_terminal_next_states).to(device)\n",
    "        next_q_values[non_terminal_mask] = torch.max(target_network(non_terminal_next_states), dim=1).values\n",
    "\n",
    "    target_q_values = rewards + gamma * next_q_values * (1 - dones)  # target q values according to bellman equation\n",
    "\n",
    "    loss = criterion(target_q_values, predicted_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "for episode in range(1, 500_001):\n",
    "    if episode % 100 == 0:\n",
    "        epsilon = max(epsilon * 0.99, 0.05)  # decay the exploration chance\n",
    "\n",
    "    game = Game()  # each episode starts a new game; the deck is NOT re-used.\n",
    "    while not game.is_over:\n",
    "        state = get_game_state(game)\n",
    "        action = choose_action(epsilon, state)\n",
    "\n",
    "        game.take_action(action)\n",
    "\n",
    "        reward = game.score\n",
    "        done = int(game.is_over)\n",
    "\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = get_game_state(game)\n",
    "\n",
    "        memory.append((state, action, reward, done, next_state))\n",
    "\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
